{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3469c5-6ca4-4a2d-b2ee-e4c6a2c7afb4",
   "metadata": {},
   "source": [
    "# Integrating ClickHouse with SageMaker\n",
    "\n",
    "This notebook demonstrates how to run a SageMaker job which pulls data from a ClickHouse database into a job without requiring an intermediate step such as S3. This eliminates some of the overheads of exporting data to S3 then ingesting into SageMaker.\n",
    "\n",
    "This example uses Apache Spark to accomplish this. I followed the steps at https://clickhouse.com/docs/en/integrations/apache-spark/spark-native-connector to complete this. No custom container was set up, this is using SageMaker's built-in functionality with ClickHouse's JAR files.\n",
    "\n",
    "My ClickHouse database is set up using the NYC Taxi dataset with 20 million records. It is hosted on an EC2 instance in my VPC.\n",
    "\n",
    "\n",
    "To demonstrate basic connectivity in Python, we can connect with the `clickhouse-connect` library as a test.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Note: Connecting and pulling data from ClickHouse into SageMaker isn't just limited to Spark. Any library which can read/query a database and has the correct connectivity will work as the connectivity is the same. SageMaker acts similar to a managed container orchestration platform here, it spins up the instances, pulls containers, runs the job and if data is to be ingested to S3, will put to S3 from local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acc867a-0c05-4feb-b216-cc681f9df72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clickhouse-connect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ebac5a-9098-4cfd-899f-0d43b7c311ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import clickhouse_connect\n",
    "\n",
    "client = clickhouse_connect.get_client(host='db.clickhouse.local', username='default', password='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf606e36-aebd-42d7-ba69-f0ca61fc0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.command(\"SELECT AVG(fare_amount) FROM trips\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9357181-f15d-428e-ac76-219484a0dd5e",
   "metadata": {},
   "source": [
    "With this, we now set up the PySpark code to read from ClickHouse. This sample script performs a SELECT statement in SparkSQL to get the data, prints the first 5 rows of the dataframe and then writes them to S3.\n",
    "\n",
    "I could set up a transform or perform other processing here if required, however I have not done this. \n",
    "\n",
    "The below code doesn't use any of SageMaker Processing's features such as S3 for input. Spark manages the input and output process. This is done to reduce end-to-end time as I do not need to be concerned with SageMaker downloading/uploading data, instead it happens in my job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5d7af5-d32c-412a-a330-aa83bb6eef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/process.py\n",
    "import pyspark\n",
    "import sys\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# create spark session\n",
    "spark = SparkSession.builder.appName('ClickHouses').getOrCreate()\n",
    "\n",
    "# set spark config. note user, password, database and host\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse\", \"com.clickhouse.spark.ClickHouseCatalog\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.host\", \"db.clickhouse.local\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.protocol\", \"http\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.http_port\", \"8123\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.user\", \"default\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.password\", \"\")\n",
    "spark.conf.set(\"spark.sql.catalog.clickhouse.database\", \"default\")\n",
    "spark.conf.set(\"spark.clickhouse.write.format\", \"json\")\n",
    "\n",
    "df = spark.sql(\"SHOW DATABASES\")\n",
    "df.show()\n",
    "\n",
    "df = spark.sql(\"select fare_amount, pickup_date, pickup_datetime, dropoff_date, dropoff_datetime, pickup_longitude, pickup_latitude, dropoff_latitude, dropoff_longitude, passenger_count, trip_distance from clickhouse.default.trips\")\n",
    "df.show(5)\n",
    "\n",
    "df.write.format('parquet').mode('Overwrite').save('s3://andjsmi-data-testing/clickout/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cf1d0f-3b92-4a52-97e0-92385e6257db",
   "metadata": {},
   "source": [
    "With the PySpark code written, I can now create the job configuration. The below code will set up a SageMaker Spark Processing Job with the code. This job runs in a VPC as configuration as to access a resource in a VPC without internet. If using ClickHouse Cloud, then this would not be required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e259b415-9bc0-4b02-b889-54c7820bf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "network_config = NetworkConfig(\n",
    "    subnets=['subnet-0761871837d46649a'],\n",
    "    security_group_ids=['sg-00f312b2360612d64']\n",
    ")\n",
    "\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name='clickspark',\n",
    "    framework_version='3.3',\n",
    "    role=sagemaker.get_execution_role(),\n",
    "    instance_count=4,\n",
    "    instance_type='ml.c5.4xlarge',\n",
    "    network_config=network_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad55b29e-490e-4a88-ac68-42893f554df2",
   "metadata": {},
   "source": [
    "Connecting to ClickHouse requires that the ClickHouse Spark Runtime and JDBC drivers are included. The below mentioned JAR files are downloaded from the Maven repository and work with this container Spark dependencies.\n",
    "\n",
    "When running the below cells, the output will show logs for the job and the nodes that it runs on.\n",
    "\n",
    "This process will start up the specified instances to create a Spark cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3945c190-f9d3-4201-981c-ceae00700c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_processor.run(\n",
    "    submit_app=\"code/process.py\",\n",
    "    submit_jars=['code/clickhouse-spark-runtime-3.3_2.12-0.8.0.jar', 'code/clickhouse-jdbc-0.6.3-all.jar']\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
